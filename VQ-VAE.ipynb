{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"bd791055","cell_type":"markdown","source":"## VQGAN From Scratch","metadata":{}},{"id":"f870ab87","cell_type":"markdown","source":"#### Install Libraries","metadata":{}},{"id":"b1fe8685","cell_type":"code","source":"import torch\nimport torch.nn as nn \nimport torch.nn.functional as F \nimport torchvision\nimport torchvision.transforms as T\nfrom torch.utils.data import DataLoader, Dataset\nimport matplotlib.pyplot as plt\nimport wandb\n# from config import Config\nimport os\nfrom PIL import Image\nfrom torchvision.utils import save_image\nfrom tqdm import tqdm\nimport wandb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T06:04:29.765890Z","iopub.execute_input":"2025-08-03T06:04:29.766453Z","iopub.status.idle":"2025-08-03T06:04:29.770861Z","shell.execute_reply.started":"2025-08-03T06:04:29.766426Z","shell.execute_reply":"2025-08-03T06:04:29.770103Z"}},"outputs":[],"execution_count":2},{"id":"ef90bcc7","cell_type":"code","source":"config = Config()","metadata":{},"outputs":[],"execution_count":2},{"id":"98111733-c00a-4e84-881f-d3364cfaac75","cell_type":"code","source":"# import torch\nclass Config:\n    def __init__(self):\n        self.in_channels = 3\n        self.hidden_channels = 128\n        self.latent_dim = 256\n        self.out_channels = 3\n        self.img_size = 64 \n        self.num_embeddings = 1024 #Total Number of Embeddings in the CodeBook\n        self.embedding_dim = 256   #Dimensionality of Each CodeBook Vector\n        self.beta = 0.25           #Commitment Loss Weight\n        self.batch_size = 32\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        \n        self.epochs = 20\n        self.lr = 2e-4\n        self.project_name = \"VQ_VAE\"\n        self.save_dir = \"./saves\"\n        self.log_interval = 100\n        self.save_interval = 2\n        self.use_wandb = True\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T06:05:44.621594Z","iopub.execute_input":"2025-08-03T06:05:44.622042Z","iopub.status.idle":"2025-08-03T06:05:44.626522Z","shell.execute_reply.started":"2025-08-03T06:05:44.622018Z","shell.execute_reply":"2025-08-03T06:05:44.626021Z"}},"outputs":[],"execution_count":7},{"id":"229f1eaf-708f-4527-bfa7-75c77d3c75d6","cell_type":"code","source":"config = Config()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T06:05:45.589080Z","iopub.execute_input":"2025-08-03T06:05:45.589378Z","iopub.status.idle":"2025-08-03T06:05:45.592917Z","shell.execute_reply.started":"2025-08-03T06:05:45.589355Z","shell.execute_reply":"2025-08-03T06:05:45.592223Z"}},"outputs":[],"execution_count":8},{"id":"8b78850f","cell_type":"markdown","source":"## Network Architecture","metadata":{}},{"id":"5a58bc27","cell_type":"markdown","source":"### Helper Class for Residual Connection and Attention Mechanism","metadata":{}},{"id":"950ce138","cell_type":"code","source":"class ResBlock(nn.Module):\n    ''' \n        Residual Block with GroupNormalization.\n        This will help in building Deeper Networks without Diminishing Gradient\n    '''\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        \n        self.block = nn.Sequential(\n            nn.GroupNorm(num_groups=32, num_channels=in_channels),\n            nn.SiLU(),\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1),\n            nn.GroupNorm(num_groups=32, num_channels=out_channels),\n            nn.SiLU(),\n            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n        )\n        \n        if in_channels != out_channels:\n            self.residual_connection = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n        else:\n            self.residual_connection = nn.Identity()\n    \n    def forward(self, x):\n        return self.residual_connection(x) + self.block(x)\n    \n    \nclass AttentionBlock(nn.Module):\n    ''' \n        A Self-Attention Block,\n        It will help the model focus on relevant parts of the image\n    '''\n    def __init__(self, channels):\n        super().__init__()\n        self.norm = nn.GroupNorm(num_groups=32, num_channels=channels)\n        self.qkv = nn.Conv2d(channels, out_channels=channels*3, kernel_size=1) #Create Q,K,V matrices\n        self.out_proj = nn.Conv2d(channels, channels, kernel_size=1)\n        \n    def forward(self, x):\n        B, C, H, W = x.shape #Get the Batch_size, Channels, Height, Width\n        h = self.norm(x)\n        q, k, v = self.qkv(h).chunk(3, dim = 1)\n        \n        #Reshape the Q,K,V for Attention Computation\n        q = q.view(B, C, H * W).permute(0, 2, 1)  #Shape: [B, H*W, C]\n        k = k.view(B, C, H * W)                   #Shape: [B, C, H*W]\n        v = v.view(B, C, H * W).permute(0, 2, 1)  #Shape: [B, H*W, C]\n        \n        #Compute Attention Weights\n        weight = torch.einsum('bic, bcj->bij', q, k)*(C ** -0.5)\n        logits = F.softmax(weight, dim = -1)\n        \n        #Attend to Values\n        h = torch.einsum('bij, bjc->bic', logits, v)\n        h = h.permute(0, 2, 1).view(B, C, H, W)  #Reshape back to iamge format\n        \n        return x + self.out_proj(h)\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T06:05:47.402796Z","iopub.execute_input":"2025-08-03T06:05:47.403364Z","iopub.status.idle":"2025-08-03T06:05:47.411740Z","shell.execute_reply.started":"2025-08-03T06:05:47.403303Z","shell.execute_reply":"2025-08-03T06:05:47.411006Z"}},"outputs":[],"execution_count":9},{"id":"646feec6","cell_type":"markdown","source":"### Encoder of the VAE","metadata":{}},{"id":"f70f8f78","cell_type":"code","source":"class Encoder(nn.Module):\n    ''' \n        Main VQ-VAE Encoder class with Residual & Attention Blocks\n    '''\n    def __init__(self, config):\n        super().__init__()\n        \n        #Initial Convolution to Increase Channel Dimension\n        self.conv_in = nn.Conv2d(config.in_channels, config.hidden_channels, kernel_size=3, stride=1, padding=1)\n        \n        #DownSampling Path\n        self.down = nn.Sequential(\n            ResBlock(config.hidden_channels, config.hidden_channels),\n            nn.Conv2d(config.hidden_channels, config.hidden_channels, kernel_size=4, stride=2, padding=1),  #Shape: [64--->32]\n            ResBlock(config.hidden_channels, config.hidden_channels),\n            nn.Conv2d(config.hidden_channels, config.hidden_channels, kernel_size=4, stride = 2, padding=1),\n        )\n        \n        #Attention Is All It Needs\n        self.mid = nn.Sequential(\n            ResBlock(config.hidden_channels, config.hidden_channels),\n            AttentionBlock(config.hidden_channels),\n            ResBlock(config.hidden_channels, config.hidden_channels)\n        )\n        \n        #Final Projection to the Latent Dimension\n        self.conv_out = nn.Sequential(\n            nn.GroupNorm(num_groups=32, num_channels=config.hidden_channels),\n            nn.SiLU(),\n            nn.Conv2d(config.hidden_channels, config.latent_dim, kernel_size=1)\n        )\n    \n    def forward(self, x):\n        #Initial Convolution\n        x = self.conv_in(x)\n        #DownSampling \n        x = self.down(x)\n        #Give IT ATTENTION\n        x = self.mid(x)\n        #Final Projection\n        output = self.conv_out(x)\n        \n        return output\n        \n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T06:05:57.270796Z","iopub.execute_input":"2025-08-03T06:05:57.271238Z","iopub.status.idle":"2025-08-03T06:05:57.277731Z","shell.execute_reply.started":"2025-08-03T06:05:57.271212Z","shell.execute_reply":"2025-08-03T06:05:57.277045Z"}},"outputs":[],"execution_count":11},{"id":"5069e709","cell_type":"markdown","source":"- Test a Dummy Forward Pass","metadata":{}},{"id":"29570bcb-75a1-43a3-b196-66e3ea303789","cell_type":"code","source":"x = torch.randn(1, 3, 64, 64)\nencoder = Encoder(config)\noutput = encoder(x)\nprint(\"=\"*30, f\"Input Shape: [{x.shape}]\", \"=\"*30)\nprint(\"=\"*30, f\"Output Shape: [{output.shape}]\", \"=\"*30)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T06:05:59.863050Z","iopub.execute_input":"2025-08-03T06:05:59.863283Z","iopub.status.idle":"2025-08-03T06:06:00.092101Z","shell.execute_reply.started":"2025-08-03T06:05:59.863267Z","shell.execute_reply":"2025-08-03T06:06:00.091374Z"}},"outputs":[{"name":"stdout","text":"============================== Input Shape: [torch.Size([1, 3, 64, 64])] ==============================\n============================== Output Shape: [torch.Size([1, 256, 16, 16])] ==============================\n","output_type":"stream"}],"execution_count":12},{"id":"6db95296","cell_type":"markdown","source":"### Decoder of the VAE","metadata":{}},{"id":"ca5b7798-0e82-4a25-b520-b034be1b9488","cell_type":"code","source":"class Decoder(nn.Module):\n    ''' \n        VQ-VAE Decoder class with Residual & Attention Blocks\n    '''\n    def __init__(self, config):\n        super().__init__()\n        \n        #Initial Projection from Latent_Dim to Hidden_Channel\n        self.conv_in = nn.Conv2d(config.latent_dim , config.hidden_channels, kernel_size= 3, stride= 1, padding=1)\n        \n        #Give it ATTENTION\n        self.mid = nn.Sequential(\n            ResBlock(config.hidden_channels, config.hidden_channels),\n            AttentionBlock(config.hidden_channels),\n            ResBlock(config.hidden_channels, config.hidden_channels),\n        )\n        \n        #UpSampling \n        self.up = nn.Sequential(\n            ResBlock(config.hidden_channels, config.hidden_channels),\n            nn.Upsample(scale_factor=2, mode=\"nearest\"), #Shape; [16---->32]\n            nn.Conv2d(config.hidden_channels, config.hidden_channels, kernel_size=3, stride=1, padding=1),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            ResBlock(config.hidden_channels, config.hidden_channels),\n            nn.Upsample(scale_factor=2, mode=\"nearest\"), #Shape; [32---->64]\n            nn.Conv2d(config.hidden_channels, config.hidden_channels, kernel_size=3, stride=1, padding=1),\n            nn.LeakyReLU(0.2, inplace=True)\n        )\n        \n        #Final Projection to Original Image Shape\n        self.conv_out = nn.Sequential(\n            nn.GroupNorm(num_groups=32, num_channels=config.hidden_channels),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(config.hidden_channels, config.in_channels, kernel_size=3, stride = 1, padding = 1)\n        )\n        \n    def forward(self, x):\n        #Initial Projection\n        x = self.conv_in(x)\n        \n        #ATTENTION IS ALL IT WANTs\n        x = self.mid(x)\n        \n        #UPSAMPLE THE Input\n        x = self.up(x)\n        \n        #Final Projection\n        out = self.conv_out(x)\n        \n        return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T06:06:04.842360Z","iopub.execute_input":"2025-08-03T06:06:04.842631Z","iopub.status.idle":"2025-08-03T06:06:04.849460Z","shell.execute_reply.started":"2025-08-03T06:06:04.842611Z","shell.execute_reply":"2025-08-03T06:06:04.848829Z"}},"outputs":[],"execution_count":13},{"id":"3a50ee0f","cell_type":"markdown","source":"- Test a Dummy Forward Pass ","metadata":{}},{"id":"b077072e-9e06-4d46-94d5-cd8839d635a0","cell_type":"code","source":"dummy_input = torch.randn(4, config.latent_dim, 16, 16)\ndecoder = Decoder(config)\n\noutput = decoder(dummy_input)\nprint(\"=\"*30, f\"Input Shape: [{dummy_input.shape}]\", \"=\"*30)\nprint(\"=\"*30, f\"Output Shape: [{output.shape}]\", \"=\"*30)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T06:06:07.173151Z","iopub.execute_input":"2025-08-03T06:06:07.173459Z","iopub.status.idle":"2025-08-03T06:06:07.352588Z","shell.execute_reply.started":"2025-08-03T06:06:07.173438Z","shell.execute_reply":"2025-08-03T06:06:07.351908Z"}},"outputs":[{"name":"stdout","text":"============================== Input Shape: [torch.Size([4, 256, 16, 16])] ==============================\n============================== Output Shape: [torch.Size([4, 3, 64, 64])] ==============================\n","output_type":"stream"}],"execution_count":14},{"id":"d6326700","cell_type":"markdown","source":"### Vector Quantization","metadata":{}},{"id":"d0e0173c","cell_type":"markdown","source":"- Instead of passing continuous latent vectors directly to the decoder, we map each latent vector to the nearest codebook embedding.\n\n**This Helps in:-**\n\n- Compressing Information\n- Introducing Discretization(which GANs Like a LOT)\n- Encouraging Disentangled Represations{When a model learns to separate the high-level, abstract concepts in data into distinct and independent factors within its latent space}","metadata":{}},{"id":"588e1515","cell_type":"code","source":"class VectorQuantizer(nn.Module):\n    ''' \n        This will take latent_inputs from the Encoder,\n        Flatten them to [B*H*W, C]\n        Find Nearest CodeBook vector for each spatial location,\n        Replaces each vector with the closes embedding\n    '''\n    \n    def __init__(self, config):\n        super().__init__()\n        \n        self.embedding_dim = config.embedding_dim \n        self.num_embeddings = config.num_embeddings\n        self.beta = config.beta\n        \n        #CodeBook Embeddings\n        self.embedding = nn.Embedding(config.num_embeddings, config.embedding_dim)\n        #Initialize the CodeBook vectors with UNIFORM DISTRIBUTION\n        self.embedding.weight.data.uniform_(-1 / config.num_embeddings, 1 / config.num_embeddings)\n        \n    def forward(self, z):\n        B, C, H, W = z.shape\n        \n        #Reshape the Input Latent Vector\n        z_orig = z\n        z_premuted = z.permute(0, 2, 3, 1).contiguous()  #Shape: [B, C, H, W]----->[B, H, W, C]\n        \n        #Flatten the vector\n        z_flat = z_premuted.view(-1, self.embedding_dim)  #Shape: [B, H, W, C]----->[B*H*W, C]\n        \n        #Compute the Distances to Embeddings: [B*H*W, C]  v/s [1, num_embeddings, C]\n        #It is computing Squared Euclidean Distance {||a - b||^2 = ||a||^2 + ||b||^2 - 2a@b}\n        dist = (torch.sum(z_flat**2, dim = 1, keepdim=True) + torch.sum(self.embedding.weight ** 2, dim = 1) - 2 * torch.matmul(z_flat, self.embedding.weight.t()))\n        \n        #Get the Nearest Embeddings\n        encoding_indices = torch.argmin(dist, dim = 1).unsqueeze(1) #Finds the index of the closest codebook vector for each of the input vectors\n        encodings = torch.zeros(encoding_indices.shape[0], self.num_embeddings, device = z.device)\n        #Get the ONE-HOT ENCODDDINGSSSS\n        encodings.scatter_(1, encoding_indices, 1) \n        \n        #Quantized = codebook[encoding_index] {It means fetch the Quantized for our latent_vector from the CodeBook}\n        quantized = torch.matmul(encodings, self.embedding.weight)\n        quantized = quantized.view(B, H, W, C).permute(0, 3, 1, 2).contiguous()\n        \n        #Compute the Loss\n        # print(\"Quantized Shape:\", quantized.detach().shape)\n        # print(\"z.permute Shape:\", z_orig.shape)\n        commitment_loss = F.mse_loss(z_orig.detach(), quantized )\n        embedding_loss = F.mse_loss(z_orig, quantized.detach())\n        loss = commitment_loss*self.beta + embedding_loss\n        \n        #Straight-through estimator\n        #argmin() operation has no gradient, which means the gradient from DECODER would stop here and never reach the ENCODER\n        quantized_for_decoder = z_orig + (quantized.clone() - z_orig).detach()\n        \n        return quantized_for_decoder, loss, encoding_indices.view(B, H, W)\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T06:06:10.154070Z","iopub.execute_input":"2025-08-03T06:06:10.154303Z","iopub.status.idle":"2025-08-03T06:06:10.161952Z","shell.execute_reply.started":"2025-08-03T06:06:10.154288Z","shell.execute_reply":"2025-08-03T06:06:10.161259Z"}},"outputs":[],"execution_count":15},{"id":"f57d9659","cell_type":"markdown","source":"- Test a Dummy Forward Pass ","metadata":{}},{"id":"179815a1","cell_type":"code","source":"vq = VectorQuantizer(config)\nz_e =  torch.randn(4, config.embedding_dim, 16, 16)\nz_q, vq_loss, indices = vq(z_e)\nz_q.shape\nvq_loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T06:06:11.917283Z","iopub.execute_input":"2025-08-03T06:06:11.917579Z","iopub.status.idle":"2025-08-03T06:06:11.975296Z","shell.execute_reply.started":"2025-08-03T06:06:11.917561Z","shell.execute_reply":"2025-08-03T06:06:11.974872Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"tensor(1.2584, grad_fn=<AddBackward0>)"},"metadata":{}}],"execution_count":16},{"id":"10acc166","cell_type":"markdown","source":"### VQVAE Architecture","metadata":{}},{"id":"b8092243","cell_type":"code","source":"class VQVAE(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.encoder = Encoder(config)\n        self.decoder = Decoder(config)\n        self.quantizer = VectorQuantizer(config)\n        self.loss_fn = nn.MSELoss()\n        \n    def forward(self, x):\n        z_e = self.encoder(x)\n        quantized , vq_loss, _ = self.quantizer(z_e)\n        x_reconstructed = self.decoder(quantized)\n        \n        reconstruction_loss = self.loss_fn(x_reconstructed, x)\n        total_loss = reconstruction_loss + vq_loss\n        \n        return {\n            \"x_reconstruction\": x_reconstructed,\n            \"z_e\": z_e,\n            \"quantized\": quantized,\n            \"vq_loss\":vq_loss,\n            \"reconstruction_loss\":reconstruction_loss,\n            \"total_loss\":total_loss\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T06:06:14.108466Z","iopub.execute_input":"2025-08-03T06:06:14.108754Z","iopub.status.idle":"2025-08-03T06:06:14.114531Z","shell.execute_reply.started":"2025-08-03T06:06:14.108733Z","shell.execute_reply":"2025-08-03T06:06:14.113648Z"}},"outputs":[],"execution_count":17},{"id":"f75b2dc7","cell_type":"markdown","source":"- Test a Dummy Forward Pass","metadata":{}},{"id":"7c2bfd09","cell_type":"code","source":"model = VQVAE(config)\nx = torch.randn(2, 3, config.img_size, config.img_size)\noutput = model(x)\n\nprint(f\"Reconstructed Image Shape: {output['x_reconstruction'].shape}\")\nprint(f\"Total Loss: {output['total_loss'].item():.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T06:07:23.556566Z","iopub.execute_input":"2025-08-03T06:07:23.556845Z","iopub.status.idle":"2025-08-03T06:07:23.729870Z","shell.execute_reply.started":"2025-08-03T06:07:23.556827Z","shell.execute_reply":"2025-08-03T06:07:23.728999Z"}},"outputs":[{"name":"stdout","text":"Reconstructed Image Shape: torch.Size([2, 3, 64, 64])\nTotal Loss: 1.3732\n","output_type":"stream"}],"execution_count":19},{"id":"c71f4a24","cell_type":"markdown","source":"## DataLoading","metadata":{}},{"id":"5f65f45e","cell_type":"code","source":"from torchvision.datasets import CIFAR10\n\ntransform = T.Compose([\n    T.Resize((config.img_size, config.img_size)),  #Resize to 64 x 64 for VQGAN\n    T.ToTensor(),\n    T.Normalize([0.5] * 3, [0.5] * 3)\n])\n\ndataset = CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\ndataloader = DataLoader(dataset, batch_size=config.batch_size, shuffle=True,num_workers=1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T06:07:25.875254Z","iopub.execute_input":"2025-08-03T06:07:25.875641Z","iopub.status.idle":"2025-08-03T06:07:32.325552Z","shell.execute_reply.started":"2025-08-03T06:07:25.875616Z","shell.execute_reply":"2025-08-03T06:07:32.324975Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 170M/170M [00:03<00:00, 48.5MB/s] \n","output_type":"stream"}],"execution_count":20},{"id":"639a68d1","cell_type":"markdown","source":"### Training Loop for Basic VQVAE","metadata":{}},{"id":"23897d06","cell_type":"code","source":"wandb.login(key = \"674eaa4b76549d6faa73e5c97b2106b8bbb2d8e4\")\n# config = Config()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T06:09:24.953671Z","iopub.execute_input":"2025-08-03T06:09:24.954426Z","iopub.status.idle":"2025-08-03T06:09:30.717138Z","shell.execute_reply.started":"2025-08-03T06:09:24.954397Z","shell.execute_reply":"2025-08-03T06:09:30.716490Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33matharv3105\u001b[0m (\u001b[33matharv3105-dr-a-p-j-abdul-kalam-technical-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":23},{"id":"f23244e5","cell_type":"code","source":"config.epochs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T06:09:35.366829Z","iopub.execute_input":"2025-08-03T06:09:35.367233Z","iopub.status.idle":"2025-08-03T06:09:35.371974Z","shell.execute_reply.started":"2025-08-03T06:09:35.367212Z","shell.execute_reply":"2025-08-03T06:09:35.371369Z"}},"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"20"},"metadata":{}}],"execution_count":24},{"id":"88ced4e9","cell_type":"code","source":"import torch.optim as optim\ndef train_vq(model,dataloader ,config):\n    \n    device = config.device\n    model.to(device)\n    model.train()\n    \n    optimizer = optim.Adam(model.parameters(), lr= config.lr)\n    \n    #Initialize WandB\n    if config.use_wandb:\n        wandb.init(project = config.project_name, config = config.__dict__)\n    \n    os.makedirs(config.save_dir, exist_ok=True)\n    \n    for epoch in range(config.epochs):\n        epoch_loss = 0.0\n        for batch_idx, (images, _) in enumerate(tqdm(dataloader)):\n            images = images.to(device)\n            \n            outputs = model(images)\n            loss = outputs[\"total_loss\"]\n            \n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            \n            epoch_loss += loss.item()\n            \n            #Log Metrics Every N steps\n            if batch_idx % config.log_interval == 0:\n                wandb.log({\n                    \"Total-Loss\": outputs[\"total_loss\"].item(),\n                    \"Reconstruction-Loss\":outputs[\"reconstruction_loss\"].item(),\n                    \"VQ_Loss\": outputs[\"vq_loss\"].item(),\n                    \"Epoch\":epoch\n                })\n        \n        #Save Reconstructions      \n        if epoch % config.save_interval == 0:\n            with torch.no_grad():\n                recon = outputs[\"x_reconstruction\"]\n                #Save Images\n                save_image(images[:8], os.path.join(config.save_dir, f\"input_{epoch}.png\"), nrow = 4, normalize = True)\n                save_image(recon[:8], os.path.join(config.save_dir, f\"recon_{epoch}.png\"), nrow = 4, normalize = True)\n\n                if config.use_wandb:\n                    \n                    wandb.log({\n                         \"Input\": [wandb.Image(images[0])], \n                         \"Reconstruction\": [wandb.Image(recon[0])]\n                        })\n            checkpoint = {\n                \"model_state_dict\":model.state_dict(),\n                \"optimizer_state_dict\": optimizer.state_dict(),\n                \"epoch\":epoch,\n                \"loss\": epoch_loss / len(dataloader)\n            }\n            ckpt_path = os.path.join(config.save_dir, f\"VQVAE_epoch_{epoch}.pth\")\n            torch.save(checkpoint, ckpt_path)\n            print(f\"Saved to Path: {ckpt_path}\")\n            \n                \n        print(f\"Epoch[{epoch + 1}/ {config.epochs}]|| Loss: {epoch_loss / len(dataloader) :.4f}\")\n    \n    wandb.finish()\n    \n    \n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T06:28:32.667217Z","iopub.execute_input":"2025-08-03T06:28:32.667658Z","iopub.status.idle":"2025-08-03T06:28:32.679477Z","shell.execute_reply.started":"2025-08-03T06:28:32.667628Z","shell.execute_reply":"2025-08-03T06:28:32.678716Z"}},"outputs":[],"execution_count":27},{"id":"a52a6abc","cell_type":"code","source":"# checkpoint = {\n#     \"model_state_dict\": model.state_dict(),\n# }\n\n# torch.save(checkpoint, \"./saves/checkpoint.pth\")","metadata":{},"outputs":[],"execution_count":70},{"id":"2afb1974","cell_type":"code","source":"# config.use_wandb = True","metadata":{},"outputs":[],"execution_count":65},{"id":"e2423104","cell_type":"code","source":"model = VQVAE(config)\ntrain_vq(model, dataloader, config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T06:30:24.678001Z","iopub.execute_input":"2025-08-03T06:30:24.678284Z","iopub.status.idle":"2025-08-03T08:05:03.344114Z","shell.execute_reply.started":"2025-08-03T06:30:24.678267Z","shell.execute_reply":"2025-08-03T08:05:03.343528Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Finishing previous runs because reinit is set to 'default'."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Reconstruction-Loss</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Total-Loss</td><td>█▆▃▂▂▂▂▂▁▂▁▁▁▁▁▁</td></tr><tr><td>VQ_Loss</td><td>▄█▄▃▃▃▂▂▂▂▂▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>0</td></tr><tr><td>Reconstruction-Loss</td><td>0.01707</td></tr><tr><td>Total-Loss</td><td>0.03456</td></tr><tr><td>VQ_Loss</td><td>0.01749</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">skilled-snowball-6</strong> at: <a href='https://wandb.ai/atharv3105-dr-a-p-j-abdul-kalam-technical-university/VQ_VAE/runs/qx51j1b2' target=\"_blank\">https://wandb.ai/atharv3105-dr-a-p-j-abdul-kalam-technical-university/VQ_VAE/runs/qx51j1b2</a><br> View project at: <a href='https://wandb.ai/atharv3105-dr-a-p-j-abdul-kalam-technical-university/VQ_VAE' target=\"_blank\">https://wandb.ai/atharv3105-dr-a-p-j-abdul-kalam-technical-university/VQ_VAE</a><br>Synced 5 W&B file(s), 2 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250803_061445-qx51j1b2/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.20.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250803_063024-pv8vq2cc</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/atharv3105-dr-a-p-j-abdul-kalam-technical-university/VQ_VAE/runs/pv8vq2cc' target=\"_blank\">royal-eon-7</a></strong> to <a href='https://wandb.ai/atharv3105-dr-a-p-j-abdul-kalam-technical-university/VQ_VAE' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/atharv3105-dr-a-p-j-abdul-kalam-technical-university/VQ_VAE' target=\"_blank\">https://wandb.ai/atharv3105-dr-a-p-j-abdul-kalam-technical-university/VQ_VAE</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/atharv3105-dr-a-p-j-abdul-kalam-technical-university/VQ_VAE/runs/pv8vq2cc' target=\"_blank\">https://wandb.ai/atharv3105-dr-a-p-j-abdul-kalam-technical-university/VQ_VAE/runs/pv8vq2cc</a>"},"metadata":{}},{"name":"stderr","text":"100%|██████████| 1563/1563 [04:41<00:00,  5.55it/s]\n","output_type":"stream"},{"name":"stdout","text":"Saved to Path: ./saves/VQVAE_epoch_0.pth\nEpoch[1/ 20]|| Loss: 0.1333\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1563/1563 [04:38<00:00,  5.61it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch[2/ 20]|| Loss: 0.0287\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1563/1563 [04:40<00:00,  5.57it/s]\n","output_type":"stream"},{"name":"stdout","text":"Saved to Path: ./saves/VQVAE_epoch_2.pth\nEpoch[3/ 20]|| Loss: 0.0180\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1563/1563 [04:44<00:00,  5.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch[4/ 20]|| Loss: 0.0148\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1563/1563 [04:44<00:00,  5.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"Saved to Path: ./saves/VQVAE_epoch_4.pth\nEpoch[5/ 20]|| Loss: 0.0129\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1563/1563 [04:44<00:00,  5.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch[6/ 20]|| Loss: 0.0116\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1563/1563 [04:44<00:00,  5.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"Saved to Path: ./saves/VQVAE_epoch_6.pth\nEpoch[7/ 20]|| Loss: 0.0107\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1563/1563 [04:44<00:00,  5.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch[8/ 20]|| Loss: 0.0101\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1563/1563 [04:44<00:00,  5.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"Saved to Path: ./saves/VQVAE_epoch_8.pth\nEpoch[9/ 20]|| Loss: 0.0095\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1563/1563 [04:44<00:00,  5.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch[10/ 20]|| Loss: 0.0089\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1563/1563 [04:44<00:00,  5.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"Saved to Path: ./saves/VQVAE_epoch_10.pth\nEpoch[11/ 20]|| Loss: 0.0082\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1563/1563 [04:44<00:00,  5.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch[12/ 20]|| Loss: 0.0080\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1563/1563 [04:44<00:00,  5.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"Saved to Path: ./saves/VQVAE_epoch_12.pth\nEpoch[13/ 20]|| Loss: 0.0075\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1563/1563 [04:43<00:00,  5.51it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch[14/ 20]|| Loss: 0.0073\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1563/1563 [04:43<00:00,  5.52it/s]\n","output_type":"stream"},{"name":"stdout","text":"Saved to Path: ./saves/VQVAE_epoch_14.pth\nEpoch[15/ 20]|| Loss: 0.0072\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1563/1563 [04:42<00:00,  5.53it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch[16/ 20]|| Loss: 0.0069\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1563/1563 [04:43<00:00,  5.51it/s]\n","output_type":"stream"},{"name":"stdout","text":"Saved to Path: ./saves/VQVAE_epoch_16.pth\nEpoch[17/ 20]|| Loss: 0.0067\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1563/1563 [04:43<00:00,  5.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch[18/ 20]|| Loss: 0.0066\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1563/1563 [04:43<00:00,  5.51it/s]\n","output_type":"stream"},{"name":"stdout","text":"Saved to Path: ./saves/VQVAE_epoch_18.pth\nEpoch[19/ 20]|| Loss: 0.0064\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1563/1563 [04:43<00:00,  5.50it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch[20/ 20]|| Loss: 0.0061\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇█████</td></tr><tr><td>Reconstruction-Loss</td><td>█▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Total-Loss</td><td>█▅▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>VQ_Loss</td><td>█▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>19</td></tr><tr><td>Reconstruction-Loss</td><td>0.00301</td></tr><tr><td>Total-Loss</td><td>0.00574</td></tr><tr><td>VQ_Loss</td><td>0.00273</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">royal-eon-7</strong> at: <a href='https://wandb.ai/atharv3105-dr-a-p-j-abdul-kalam-technical-university/VQ_VAE/runs/pv8vq2cc' target=\"_blank\">https://wandb.ai/atharv3105-dr-a-p-j-abdul-kalam-technical-university/VQ_VAE/runs/pv8vq2cc</a><br> View project at: <a href='https://wandb.ai/atharv3105-dr-a-p-j-abdul-kalam-technical-university/VQ_VAE' target=\"_blank\">https://wandb.ai/atharv3105-dr-a-p-j-abdul-kalam-technical-university/VQ_VAE</a><br>Synced 5 W&B file(s), 20 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250803_063024-pv8vq2cc/logs</code>"},"metadata":{}}],"execution_count":28},{"id":"6f32d12a","cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}